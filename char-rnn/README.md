## Character-level RNN for text generation

A quick implementation of a two-layer LSTM in Keras, inspired by http://karpathy.github.io/2015/05/21/rnn-effectiveness/
Some outputs when trained on *The Adventures of Sherlock Holmes*, grabbed at http://www.gutenberg.org/:
* **Sample sequence:** id i.

“‘hampshire. charming rural place. the copper beeches, five miles on the
far side of winchest

**Output:**  cotsings,
and nearly invested to our dog, and he was pericate knowing anything
at frincher. the facts was madring the ttrick off.”

i vaided by a regular of drove to terribly, and as he might fell up with a chall place, and
there
the codtuls alones filsing dawve as your successver,” said i, “he’s in the public o1g hotel mr. norton?’s
“are yourself in check merest and hearties. you reach,” in
heapting when she hore three meanp police-clays of police. “they
took on, but what have you interested?”

* **Sample sequence:** emotion. “oh, sir,” he cried, “can you tell me
where it went to?”

“it came here.”

“here?”

“yes, an

**Output:** nd even even
placed in dinfer as a pocket
into the steps. irene other point would be his
friend with this ring to the
glass and burnd of wonderful
lair and recalled. i started to
right in london,” he answered.

“it is not as incuntifful unthis fine.”

“no doubt,” said holmes, the street, and likely that had had refired,
recalled downward and
ground, and had for-sight at findings leave the fire that he was not a lady of my tleep
by the doctor of his custom, took out of the goose,’ said he.

“you 